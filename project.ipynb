{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import urllib.parse\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFile(name):\n",
    "    directory = str(os.getcwd())\n",
    "    filepath = os.path.join(directory, name)\n",
    "    with open(filepath,encoding=\"utf8\") as f:\n",
    "        data = f.readlines()\n",
    "    data = list(set(data))\n",
    "    result = []\n",
    "    for d in data:\n",
    "        d = str(urllib.parse.unquote(d))   #converting url encoded data to simple string\n",
    "        result.append(d)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take the data from kaggle sentiment competition : https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    badQueries = loadFile('badqueries.txt')\n",
    "    validQueries = loadFile('goodqueries.txt')\n",
    "    badQueries = list(set(badQueries))\n",
    "    validQueries = list(set(validQueries))\n",
    "    allQueries = badQueries + validQueries\n",
    "    yBad = [1 for i in range(0, len(badQueries))]  #labels, 1 for malicious and 0 for clean\n",
    "    yGood = [0 for i in range(0, len(validQueries))]\n",
    "    y = yBad + yGood\n",
    "    queries = allQueries\n",
    "    \n",
    "    test = []\n",
    "    for d in queries:\n",
    "        test4 = d.replace('=', ' ')\n",
    "        test5 = test4.replace('?', ' ')\n",
    "        test6 = test5.replace('/', ' ')\n",
    "        test7 = test6.replace('/*', '')\n",
    "        test.append(test6)\n",
    "    \n",
    "    X = test\n",
    "    \n",
    "    X_test = list()\n",
    "    Y_test = list()\n",
    "    \n",
    "    tag2idx = dict(enumerate(sorted(set(y))))\n",
    "    \n",
    "    list_vocab = list(set(np.hstack(map(lambda sent: sent.split(), X))))\n",
    "    list_vocab = map(lambda wrd: wrd.lower(), list_vocab)\n",
    "    \n",
    "    word2idx = dict()\n",
    "    word2idx[\"<PAD>\"] = 0\n",
    "    idx_wrd = 1\n",
    "    for word in list_vocab:\n",
    "        idx = word2idx.get(word, None)\n",
    "        \n",
    "        if not idx:\n",
    "            word2idx[word] = idx_wrd\n",
    "            idx_wrd += 1\n",
    "    \n",
    "    #represents the unknown words\n",
    "    word2idx[\"<UNK>\"] = idx_wrd\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42) #splitting data\n",
    "    return X_train, Y_train, X_test, Y_test, word2idx, tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dc horror comics \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, word2idx, tag2idx = get_data()\n",
    "print(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "448995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "print(type(idx2word))\n",
    "print(len(idx2word))\n",
    "print(idx2word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vocab file for visualization\n",
    "df_idx_word = pd.DataFrame.from_dict(idx2word, orient=\"index\")\n",
    "df_idx_word.to_csv(path_or_buf=\"sentiment_lstm_vocab.tsv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95095 95095\n"
     ]
    }
   ],
   "source": [
    "# length of train data\n",
    "print (len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# tag2idx\n",
    "print (tag2idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 448995\n",
      "Tags #  2\n"
     ]
    }
   ],
   "source": [
    "N = len(X_test)\n",
    "V = len(word2idx)\n",
    "K = len(tag2idx)\n",
    "print (\"vocabulary size:\", V)\n",
    "print (\"Tags # \", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " javascript tsweb.doc\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "# sample of data\n",
    "print (X_train[0], Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to indices in the train/test data\n",
    "def convert_words_to_indices(data):\n",
    "    list_idx_data = list()\n",
    "    for sentence in data:\n",
    "        list_words = list()\n",
    "        for word in sentence.split():\n",
    "            word = word.lower()\n",
    "            if \"\\x00\" in word:\n",
    "                print(\"123 test\", word)\n",
    "                word = word.replace(\"\\x00\", \"\")\n",
    "            try:\n",
    "                list_words.append(word2idx[word])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        list_idx_data.append(list_words)\n",
    "    return list_idx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 test xxpathxx\u0000\n",
      "123 test java\u0000script:alert(\\\"xss\\\")>\n",
      "123 test xxpathxx\u0000&cmd\n",
      "123 test \u0000\n",
      "123 test xxpathxx\u0000\n",
      "123 test passwd\u0000.html\n",
      "123 test xsstest\u0000\"<>'\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331999766.asp\u0000.cfm\n",
      "123 test xxpathxx\u0000\n",
      "123 test coldfusion_apache_double_null_info_disclosure.nasl-1332008609\u0000.cfm\n",
      "123 test \u0000>alert&lpar;1&rpar;\n",
      "123 test \\..\\..\\..\\..\\..\\..\\..\\..\\..\\..\\etc\\passwd\u0000\n",
      "123 test \u0000..\n",
      "123 test xxpathxx\u0000\n",
      "123 test xxpathxx\u0000\n",
      "123 test dir\u0000\n",
      "123 test \u0000\"><script>alert(1)<\n",
      "123 test shadow\u0000\n",
      "123 test \u0000\n",
      "123 test xxpathxx\u0000\n",
      "123 test \u0000\u0000v\u0000\u0000>confirm(\n",
      "123 test (\u00000000\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1332000199.asp\u0000.cfm\n",
      "123 test 'javascript0x00:void(0)\u0000\n",
      "123 test *\u0000\n",
      "123 test \u0000*\n",
      "123 test <s\u0000c\u0000r\u0000\u0000ip\u0000t>confirm(0);<\n",
      "123 test s\u0000c\u0000r\u0000\u0000ip\u0000t>\n",
      "123 test boot.ini\u0000.jpg\n",
      "123 test `\u0000`\n",
      "123 test xxpathxx\u0000\n",
      "123 test ..\\..\\..\\..\\..\\..\\..\\..\\..\\..\\etc\\passwd\u0000\n",
      "123 test <scr\u0000ipt>alert(\\\"xss\\\")<\n",
      "123 test scr\u0000ipt>\n",
      "123 test `\u0000`&newline;\n",
      "123 test 1+len(@@servername)>1;\u0000\n",
      "123 test char),0x20),0x3a736a763a)#\u0000\n",
      "123 test coldfusion_apache_double_null_info_disclosure.nasl-1331923193\u0000.cfm\n",
      "123 test ><scri\u0000pt>confirm(0);<\n",
      "123 test scri\u0000pt>\n",
      "123 test <\u0000>\n",
      "123 test id\u0000|\n",
      "123 test `\u0000`\n",
      "123 test id\u0000\n",
      "123 test if(if((name)like(0x61646d696e),1,0),if(mid((password),1,1)like(0x61),id,0),0);\u0000\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1332008326.asp\u0000.cfm\n",
      "123 test <scr\u0000ipt>confirm(document.location)<\n",
      "123 test scr\u0000ipt>\n",
      "123 test `\u0000`&newline;\n",
      "123 test \u0000\u0000v\u0000\u0000>confirm(\n",
      "123 test (\u00000000\n",
      "123 test <script\u0000>alert(1)<\n",
      "123 test script\u0000>\n",
      "123 test '\\u0061lert(1)'>\u0000\n",
      "123 test \u0000confirm(2);\n",
      "123 test dir\u0000|\n",
      "123 test \\..\\..\\..\\..\\..\\..\\..\\..\\..\\..\\etc\\shadow\u0000\n",
      "123 test passwd\u0000.jpg\n",
      "123 test \u0000>confirm&lpar;1&rpar;\n",
      "123 test +\u0000\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331919163.asp\u0000.cfm\n",
      "123 test \u0000\u0000\u0000\u0000\u0000\u0002\u0003�)n.�,(q����+�u�())���\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1332000208.asp\u0000.cfm\n",
      "123 test xxpathxx\u0000\n",
      "123 test \u0000\n",
      "123 test \"\\\"><scri\u0000pt>confirm(0);<\n",
      "123 test scri\u0000pt>\",\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331904662.asp\u0000.cfm\n",
      "123 test xxpathxx\u0000\n",
      "123 test \u0000\n",
      "123 test \u0000\n",
      "123 test script\u0000testtest>\n",
      "123 test xxpathxx\u0000\n",
      "123 test <scri\u0000pt>confirm(1);<\n",
      "123 test scri\u0000pt>\n",
      "123 test coldfusion_apache_double_null_info_disclosure.nasl-1331918984\u0000.cfm\n",
      "123 test ..\\..\\..\\..\\..\\..\\..\\..\\..\\..\\etc\\shadow\u0000\n",
      "123 test ..\\..\\..\\..\\..\\..\\..\\..\\..\\..\\boot.ini\u0000\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331922682.asp\u0000.cfm\n",
      "123 test java\u0000script:confirm(document.location)>\n",
      "123 test <scri\u0000pt>alert(1);<\n",
      "123 test scri\u0000pt>\n",
      "123 test %\\..%\\..%\\..%\\..%\\..%\\..%\\..%\\..%\\..%\\..%\\..%\\..%\\..%\\..\u0000\n",
      "123 test passwd\u0000\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1332010392.asp\u0000.cfm\n",
      "123 test <scri\u0000ipt>confirm(0);<\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331999902.asp\u0000.cfm\n",
      "123 test �\u0000\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331999938.asp\u0000.cfm\n",
      "123 test scri\u0000pt>\n",
      "123 test xxpathxx\u0000\n",
      "123 test boot.ini\u0000.html\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331999931.asp\u0000.cfm\n",
      "123 test <scri\u0000pt>confirm(0);<\n",
      "123 test scri\u0000pt>\n",
      "123 test onload\u0000\n",
      "123 test \u0000locatio\u0000n\n",
      "123 test nam\u0000e\n",
      "123 test hosts\u0000\n",
      "123 test \u0000\n",
      "123 test *\u0000*\n",
      "123 test *\u0000*\n",
      "123 test *\u0000*\n",
      "123 test *\u0000*\n",
      "123 test xxpathxx\u0000\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331922749.asp\u0000.cfm\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331999797.asp\u0000.cfm\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331999933.asp\u0000.cfm\n",
      "123 test \u0000\n",
      "123 test 25\\..%\\..\u0000\n",
      "123 test boot.ini\u0000\n",
      "123 test q54599877\u0000&q3916\u0000\"q3916\u0000<q3916\u0000>q54599877\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331922910.asp\u0000.cfm\n",
      "123 test \u0000\n",
      "123 test xxpathxx\u0000\n",
      "123 test xxpathxx\u0000\n",
      "123 test coldfusion_double_encoded_null_info_disclosure-1331999731.asp\u0000.cfm\n",
      "123 test xxpathxx\u0000&cmd\n"
     ]
    }
   ],
   "source": [
    "X_train = convert_words_to_indices(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58319, 258349] 0\n"
     ]
    }
   ],
   "source": [
    "# sample of data\n",
    "print (X_train[0], Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(Mi, Mo):\n",
    "        return np.random.rand(Mi, Mo)/ np.sqrt(Mi+Mo)\n",
    "    \n",
    "    def __init__(self, D, M, V, K, batch_size=2, learning_rate=0.05):\n",
    "        \"\"\"\n",
    "        D: dimensionality of word embeddings\n",
    "        M: size of hidden layer\n",
    "        V: size of vocabulary\n",
    "        K: num of output classes\n",
    "        \"\"\"\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.V = V\n",
    "        self.K = K\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.tf_session = tf.Session()\n",
    "        \n",
    "        with tf.name_scope(\"We\"):\n",
    "            self.We = tf.Variable(tf.random_uniform([self.V, self.D], -0.0001, 0.0001), name=\"We\")\n",
    "            \n",
    "            tf.summary.histogram(\"hist_We\", self.We)\n",
    "        \n",
    "        with tf.name_scope(\"f_gate\"):\n",
    "            self.Wxf = tf.Variable(LSTM.init_weights(self.D, self.M), dtype=tf.float32, name=\"Wxf\")\n",
    "            self.Whf = tf.Variable(LSTM.init_weights(self.M, self.M), dtype=tf.float32, name=\"Whf\")\n",
    "\n",
    "            tf.summary.histogram(\"hist_Wxf\", self.Wxf)\n",
    "            tf.summary.histogram(\"hist_Whf\", self.Whf)\n",
    "                \n",
    "        with tf.name_scope(\"i_gate\"):\n",
    "            self.Wxi = tf.Variable(LSTM.init_weights(self.D, self.M), dtype=tf.float32, name=\"Wxi\")\n",
    "            self.Whi = tf.Variable(LSTM.init_weights(self.M, self.M), dtype=tf.float32, name=\"Whi\")\n",
    "\n",
    "            tf.summary.histogram(\"hist_Wxi\", self.Wxi)\n",
    "            tf.summary.histogram(\"hist_Whi\", self.Whi)\n",
    "            \n",
    "        with tf.name_scope(\"o_gate\"):\n",
    "            self.Wxo = tf.Variable(LSTM.init_weights(self.D, self.M), dtype=tf.float32, name=\"Wxo\")\n",
    "            self.Who = tf.Variable(LSTM.init_weights(self.M, self.M), dtype=tf.float32, name=\"Who\")\n",
    "\n",
    "            tf.summary.histogram(\"hist_Wxo\", self.Wxo)\n",
    "            tf.summary.histogram(\"hist_Who\", self.Who)\n",
    "            \n",
    "        with tf.name_scope(\"c_hat\"):\n",
    "            self.Wxc = tf.Variable(LSTM.init_weights(self.D, self.M), dtype=tf.float32, name=\"Wxc\")\n",
    "            self.Whc = tf.Variable(LSTM.init_weights(self.M, self.M), dtype=tf.float32, name=\"Whc\")\n",
    "\n",
    "            tf.summary.histogram(\"hist_Wxc\", self.Wxc)\n",
    "            tf.summary.histogram(\"hist_Whc\", self.Whc)\n",
    "            \n",
    "        with tf.name_scope(\"biases\"):\n",
    "            self.bi = tf.Variable(tf.zeros(shape=[self.M]), dtype=tf.float32, name=\"bi\")\n",
    "            self.bo = tf.Variable(tf.zeros(shape=[self.M]), dtype=tf.float32, name=\"bo\")\n",
    "            self.bf = tf.Variable(tf.zeros(shape=[self.M]), dtype=tf.float32, name=\"bf\")\n",
    "            self.bc = tf.Variable(tf.zeros(shape=[self.M]), dtype=tf.float32, name=\"bc\")\n",
    "            \n",
    "            tf.summary.histogram(\"hist_bi\", self.bi)\n",
    "            tf.summary.histogram(\"hist_bo\", self.bo)\n",
    "            tf.summary.histogram(\"hist_bf\", self.bf)\n",
    "            tf.summary.histogram(\"hist_bc\", self.bc)\n",
    "            \n",
    "        with tf.name_scope(\"c_0\"):\n",
    "            self.c0 = tf.zeros(shape=[self.M], dtype=tf.float32, name=\"c0\")\n",
    "        \n",
    "        with tf.name_scope(\"h_0\"):\n",
    "            self.h0 = tf.zeros(shape=[self.M], dtype=tf.float32, name=\"h0\")\n",
    "        \n",
    "        self._initial_hidden_cell_states = tf.stack([self.h0, self.c0])\n",
    "        \n",
    "        with tf.name_scope(\"output_layer\"):\n",
    "            self.W_op = tf.Variable(LSTM.init_weights(self.M, self.K), dtype=tf.float32, name=\"W_op\")\n",
    "            self.b_op = tf.Variable(tf.zeros(shape=[self.K]), dtype=tf.float32, name=\"b_op\")\n",
    "            \n",
    "            tf.summary.histogram(\"hist_W_op\", self.W_op)\n",
    "            tf.summary.histogram(\"hist_b_op\", self.b_op)\n",
    "            \n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            self.input_seq = tf.placeholder(tf.int32, shape=[None, None], name=\"input_seq\")\n",
    "            self.targets  = tf.placeholder(tf.int32, shape=[None], name=\"targets\")\n",
    "        \n",
    "        self.seq_len = tf.placeholder(tf.int32, shape=[None], name=\"seq_len\")\n",
    "        self.current_batch_size = tf.placeholder(tf.int32, shape=(), name=\"batch_size\")\n",
    "        self.batch_max_len = tf.placeholder(tf.int32, shape=(), name=\"batch_max_len\")\n",
    "        \n",
    "        self.build_graph()\n",
    "\n",
    "        self.save_dir = \"train_log/sentiment_lstm_mini_batch_gd\"\n",
    "        self.model_path = os.path.join(self.save_dir, \"model_sentiment_lstm.ckpt\")\n",
    "        self.emb_path = os.path.join(self.save_dir, \"word_embedding_sentiment_lstm.npy\")\n",
    "        self.saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "        self.add_summary_file_writer()\n",
    "        self.add_summary_embedding()\n",
    "        self.train_writer.add_graph(graph=self.tf_session.graph, global_step=1)\n",
    "        \n",
    "        \n",
    "    def build_graph(self):\n",
    "        input_embeddings = self.get_embeddings(self.input_seq)\n",
    "        \n",
    "        tensor_array_py_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False, infer_shape=False, name=\"tensor_array_ho\")\n",
    "\n",
    "        loop_batch_cond = lambda tensor_array_py_x, input_embeddings, idx_sent: tf.less(idx_sent, self.current_batch_size)\n",
    "        batch_py_x, _, _ = tf.while_loop(\n",
    "            loop_batch_cond, self.loop_batch, (tensor_array_py_x, input_embeddings, 0), parallel_iterations=5, name=\"loop_\"\n",
    "        )\n",
    "\n",
    "        self.py_x = batch_py_x.concat()\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.py_x, labels=self.targets)           \n",
    "            self.loss = tf.divide(tf.reduce_sum(loss), tf.cast(self.current_batch_size, tf.float32))\n",
    "        \n",
    "        with tf.name_scope(\"train_op\"):\n",
    "            trainables = tf.trainable_variables()\n",
    "            \n",
    "            grads = tf.gradients(self.loss, trainables)\n",
    "            \n",
    "            grads, _ = tf.clip_by_global_norm(grads, clip_norm=1)\n",
    "            grad_var_pairs = zip(grads, trainables)\n",
    "            \n",
    "            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "\n",
    "            self.train_op = opt.apply_gradients(grad_var_pairs)\n",
    "        \n",
    "    def add_summary_file_writer(self):\n",
    "        print (\"Creating FileWriter\")\n",
    "        self.train_writer = tf.summary.FileWriter(self.save_dir , graph=self.tf_session.graph)\n",
    "        \n",
    "    def add_summary_embedding(self):\n",
    "        \"\"\"\n",
    "        refer https://www.tensorflow.org/how_tos/embedding_viz/\n",
    "        \"\"\"\n",
    "        print (\"Creating Embedding Projections\")\n",
    "        config = projector.ProjectorConfig()\n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = self.We.name\n",
    "        embedding.metadata_path = \"sentiment_lstm_vocab.tsv\"\n",
    "        projector.visualize_embeddings(self.train_writer, config)\n",
    "    \n",
    "\n",
    "    def get_embeddings(self, idx_input_seq):\n",
    "        return tf.nn.embedding_lookup(self.We, idx_input_seq)\n",
    "    \n",
    "    def _recurrence(self, previous_hidden_memory_tuple, x_t):\n",
    "        h_t_minus_1, c_t_minus_1 = tf.unstack(previous_hidden_memory_tuple)\n",
    "        \n",
    "        x_t = tf.reshape(x_t, [1, self.D])\n",
    "        h_t_minus_1 = tf.reshape(h_t_minus_1,[1, self.M])\n",
    "        c_t_minus_1 = tf.reshape(c_t_minus_1,[1, self.M])\n",
    "        \n",
    "        f_t = tf.nn.sigmoid(\n",
    "            tf.matmul(x_t, self.Wxf) + tf.matmul(h_t_minus_1, self.Whf) + self.bf\n",
    "        )\n",
    "        \n",
    "        i_t = tf.nn.sigmoid(\n",
    "            tf.matmul(x_t, self.Wxi) + tf.matmul(h_t_minus_1, self.Whi) + self.bi\n",
    "        )\n",
    "        \n",
    "        c_hat_t = tf.nn.tanh(\n",
    "            tf.matmul(x_t, self.Wxc) + tf.matmul(h_t_minus_1, self.Whc) + self.bc\n",
    "        )\n",
    "        \n",
    "        c_t = (f_t * c_t_minus_1) + (i_t * c_hat_t)\n",
    "        \n",
    "        o_t = tf.nn.sigmoid(\n",
    "            tf.matmul(x_t, self.Wxo) + tf.matmul(h_t_minus_1, self.Who) + self.bo\n",
    "        )\n",
    "        \n",
    "        h_t = o_t * tf.nn.tanh(c_t)\n",
    "        \n",
    "        h_t = tf.reshape(h_t, [self.M])\n",
    "        c_t = tf.reshape(c_t, [self.M])\n",
    "        \n",
    "        return tf.stack([h_t, c_t])\n",
    "    \n",
    "    def loop_batch(self, tensor_array_py_x, input_embeddings, idx_sent):\n",
    "        hidden_cell_states = tf.scan(\n",
    "            fn=self._recurrence, elems=tf.gather(input_embeddings, idx_sent), initializer=self._initial_hidden_cell_states, name=\"hidden_states\"\n",
    "        )\n",
    "        \n",
    "        h_t, c_t = tf.unstack(hidden_cell_states, axis=1)\n",
    "\n",
    "        h_t_last = tf.reshape(h_t[-1, :], [1, self.M])\n",
    "\n",
    "        py_x = tf.matmul(h_t_last, self.W_op) + self.b_op\n",
    "        \n",
    "        tensor_array_py_x = tensor_array_py_x.write(idx_sent, py_x)\n",
    "        idx_sent = tf.add(idx_sent, 1)\n",
    "        \n",
    "        return tensor_array_py_x, input_embeddings, idx_sent\n",
    "    \n",
    "    \n",
    "    def fit(self, X, Y, lr=1e-2, epochs=500):\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "        print (\"Initializing global variables\")\n",
    "        self.tf_session.run(tf.global_variables_initializer())\n",
    "        print (\"# of trainable var outside: \" + str(len(tf.trainable_variables())))\n",
    "        \n",
    "        net_epoch_step_idx = 0\n",
    "        \n",
    "        num_samples = len(X)\n",
    "        costs = list()\n",
    "        for idx_epoch in range(epochs):\n",
    "            cost_epoch = 0\n",
    "            accuracy_epoch = list()\n",
    "            \n",
    "            X_train, Y_train = shuffle(X, Y, n_samples=num_samples)\n",
    "            \n",
    "            net_epoch_step_idx = num_samples * idx_epoch\n",
    "            current_idx_sent = 0\n",
    "        \n",
    "            while current_idx_sent < len(X_train):\n",
    "                print (\"----------------------------------------------------\")\n",
    "                print (\"epoch: {}, sentence: {}\".format(idx_epoch, current_idx_sent))\n",
    "                \n",
    "                seq_len = list()\n",
    "                \n",
    "                targets = Y_train[current_idx_sent: current_idx_sent + self.batch_size]\n",
    "                \n",
    "                x = X_train[current_idx_sent: current_idx_sent + self.batch_size]\n",
    "                max_len = max([len(sentence) for sentence in x])\n",
    "                                \n",
    "                input_seq = list()\n",
    "                for index, sentence in enumerate(x):\n",
    "                    seq_len.append(len(sentence))\n",
    "                    padded_sentence = list(np.pad(sentence, (0, max_len-len(sentence)), 'constant', constant_values=0))\n",
    "                    input_seq.append(padded_sentence)\n",
    "                \n",
    "                current_batch_size = len(seq_len)\n",
    "                \n",
    "                net_epoch_step_idx += current_batch_size\n",
    "                \n",
    "                feed_dict={\n",
    "                    self.input_seq: input_seq, \n",
    "                    self.targets: targets,\n",
    "                    self.current_batch_size: current_batch_size,\n",
    "                    self.seq_len: seq_len,\n",
    "                    self.batch_max_len: max_len\n",
    "                }\n",
    "                \n",
    "                self.tf_session.run(self.train_op, feed_dict=feed_dict)\n",
    "                                \n",
    "                py_x, loss, We = self.tf_session.run([self.py_x, self.loss, self.We], feed_dict=feed_dict)\n",
    "                \n",
    "                pred = np.argmax(py_x, axis=1)\n",
    "                print (\"Y: \", targets)\n",
    "                print (\"Prediction: \", pred)\n",
    "                \n",
    "                accuracy = 0\n",
    "                for y, y_ in zip(targets, pred):\n",
    "                    if y==y_:\n",
    "                        accuracy += 1 \n",
    "                accuracy = float(accuracy)/len(pred)\n",
    "        \n",
    "                print (\"Accuracy/Batch # {}\".format(current_idx_sent), accuracy)\n",
    "                print (\"Loss/Batch # {}\".format(current_idx_sent), loss)\n",
    "                cost_epoch += loss\n",
    "                \n",
    "                accuracy_epoch.append(accuracy)\n",
    "                \n",
    "                loss_step = tf.Summary(\n",
    "                    value=[\n",
    "                        tf.Summary.Value(tag=\"loss_per_mini_batch\", simple_value=loss),\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                accuracy_step = tf.Summary(\n",
    "                    value=[\n",
    "                        tf.Summary.Value(tag=\"accuracy_per_mini_batch\", simple_value=accuracy),\n",
    "                    ]\n",
    "                )\n",
    "                    \n",
    "                self.train_writer.add_summary(loss_step, net_epoch_step_idx)\n",
    "                self.train_writer.add_summary(accuracy_step, net_epoch_step_idx)\n",
    "                \n",
    "                if net_epoch_step_idx%1000 == 0:\n",
    "                    self.save_model(step=net_epoch_step_idx)\n",
    "                    summary = self.tf_session.run(tf.summary.merge_all())\n",
    "                    self.train_writer.add_summary(summary, idx_epoch)\n",
    "                    self.train_writer.flush()\n",
    "                \n",
    "                current_idx_sent += self.batch_size\n",
    "                \n",
    "            costs.append(cost_epoch)\n",
    "            \n",
    "            print (\"---------\")\n",
    "            print (\"Cost at epoch {} is {}\".format(idx_epoch, cost_epoch))\n",
    "\n",
    "            print (\"Accuracy at epoch {} is {}\".format(idx_epoch, np.mean(accuracy_epoch)))\n",
    "            print (\"---------\")\n",
    "            \n",
    "            self.save_model(step=net_epoch_step_idx)\n",
    "            \n",
    "            summary_cost_epoch = tf.Summary(\n",
    "                value=[\n",
    "                    tf.Summary.Value(tag=\"loss/epoch\", simple_value=cost_epoch),\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            summary_accuracy_epoch = tf.Summary(\n",
    "                value=[\n",
    "                    tf.Summary.Value(tag=\"accuracy/epoch\", simple_value=np.mean(accuracy_epoch)),\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            self.train_writer.add_summary(summary_cost_epoch, idx_epoch)\n",
    "            self.train_writer.add_summary(summary_accuracy_epoch, idx_epoch)\n",
    "            summary = self.tf_session.run(tf.summary.merge_all())\n",
    "            self.train_writer.add_summary(summary, idx_epoch)\n",
    "            self.train_writer.flush()\n",
    "\n",
    "    def save_model(self, step=1):\n",
    "        self.save_embedding_matrix()\n",
    "        print (\"saving model for step\", step, \"to\", self.model_path)\n",
    "        self.saver.save(self.tf_session, self.model_path, step)\n",
    "        \n",
    "    def save_embedding_matrix(self):\n",
    "        np.save(self.emb_path, self.We.eval(self.tf_session))\n",
    "        \n",
    "    def close_session(self):\n",
    "        self.tf_session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dungpc\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FileWriter\n",
      "Creating Embedding Projections\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(300, 10, V, 2, batch_size=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing global variables\n",
      "# of trainable var outside: 15\n",
      "----------------------------------------------------\n",
      "epoch: 0, sentence: 0\n",
      "Y:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.6679547\n",
      "---------\n",
      "Cost at epoch 0 is 0.667954683303833\n",
      "Accuracy at epoch 0 is 0.85\n",
      "---------\n",
      "saving model for step 100 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 1, sentence: 0\n",
      "Y:  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.6451902\n",
      "---------\n",
      "Cost at epoch 1 is 0.6451901793479919\n",
      "Accuracy at epoch 1 is 0.85\n",
      "---------\n",
      "saving model for step 200 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 2, sentence: 0\n",
      "Y:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.62455904\n",
      "---------\n",
      "Cost at epoch 2 is 0.6245590448379517\n",
      "Accuracy at epoch 2 is 0.85\n",
      "---------\n",
      "saving model for step 300 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 3, sentence: 0\n",
      "Y:  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.60578495\n",
      "---------\n",
      "Cost at epoch 3 is 0.6057849526405334\n",
      "Accuracy at epoch 3 is 0.85\n",
      "---------\n",
      "saving model for step 400 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 4, sentence: 0\n",
      "Y:  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.5886017\n",
      "---------\n",
      "Cost at epoch 4 is 0.5886017084121704\n",
      "Accuracy at epoch 4 is 0.85\n",
      "---------\n",
      "saving model for step 500 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 5, sentence: 0\n",
      "Y:  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.5727415\n",
      "---------\n",
      "Cost at epoch 5 is 0.5727415084838867\n",
      "Accuracy at epoch 5 is 0.85\n",
      "---------\n",
      "saving model for step 600 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 6, sentence: 0\n",
      "Y:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.55791616\n",
      "---------\n",
      "Cost at epoch 6 is 0.5579161643981934\n",
      "Accuracy at epoch 6 is 0.85\n",
      "---------\n",
      "saving model for step 700 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 7, sentence: 0\n",
      "Y:  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.5437849\n",
      "---------\n",
      "Cost at epoch 7 is 0.5437849164009094\n",
      "Accuracy at epoch 7 is 0.85\n",
      "---------\n",
      "saving model for step 800 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 8, sentence: 0\n",
      "Y:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.5298967\n",
      "---------\n",
      "Cost at epoch 8 is 0.5298966765403748\n",
      "Accuracy at epoch 8 is 0.85\n",
      "---------\n",
      "saving model for step 900 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 9, sentence: 0\n",
      "Y:  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.51558995\n",
      "saving model for step 1000 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Cost at epoch 9 is 0.5155899524688721\n",
      "Accuracy at epoch 9 is 0.85\n",
      "---------\n",
      "saving model for step 1000 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 10, sentence: 0\n",
      "Y:  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.49987134\n",
      "---------\n",
      "Cost at epoch 10 is 0.4998713433742523\n",
      "Accuracy at epoch 10 is 0.85\n",
      "---------\n",
      "saving model for step 1100 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 11, sentence: 0\n",
      "Y:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.48159108\n",
      "---------\n",
      "Cost at epoch 11 is 0.4815910756587982\n",
      "Accuracy at epoch 11 is 0.85\n",
      "---------\n",
      "saving model for step 1200 to train_log/sentiment_lstm_mini_batch_gd\\model_sentiment_lstm.ckpt\n",
      "----------------------------------------------------\n",
      "epoch: 12, sentence: 0\n",
      "Y:  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "Prediction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy/Batch # 0 0.85\n",
      "Loss/Batch # 0 0.4611397\n",
      "---------\n",
      "Cost at epoch 12 is 0.4611397087574005\n",
      "Accuracy at epoch 12 is 0.85\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lstm.fit(X_train[:100], Y_train[:100])\n",
    "finally:\n",
    "    print (\"Closing Session\")\n",
    "    lstm.close_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
